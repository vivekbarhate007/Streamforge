# StreamForge â€” End-to-End Data Engineering Pipeline

A production-quality, full-stack data engineering project demonstrating batch and streaming data pipelines with a modern analytics dashboard.

## ğŸ¯ Project Overview

StreamForge is a complete data engineering solution that:
- **Ingests** real-time events via Kafka and batch transactions via CSV
- **Processes** data using Apache Spark (streaming + batch)
- **Stores** raw data in a data lake (PostgreSQL)
- **Transforms** data using dbt to build a star schema
- **Validates** data quality with Great Expectations
- **Serves** metrics through a FastAPI backend
- **Visualizes** KPIs in a Next.js dashboard

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer  â”‚â”€â”€â”
â”‚  (Python)   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch CSV  â”‚  â”‚    â”‚    Kafka     â”‚    â”‚    Spark     â”‚
â”‚   Files    â”‚â”€â”€â”¼â”€â”€â”€â–¶â”‚   Broker     â”‚â”€â”€â”€â–¶â”‚  Streaming  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                              â”‚
                 â”‚                              â–¼
                 â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                        â”‚ PostgreSQL  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Data Lake   â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                            â”‚                            â”‚
                    â–¼                            â–¼                            â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     dbt      â”‚            â”‚   Great      â”‚            â”‚   FastAPI    â”‚
            â”‚ Transformationsâ”‚          â”‚ Expectations â”‚            â”‚    API      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                            â”‚                            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚   Next.js    â”‚
                                          â”‚   Dashboard  â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Make (optional, for convenience commands)

### One-Command Setup

```bash
# Clone the repository
git clone <repository-url>
cd StreamForge

# Start all services
make up
# OR
docker compose up --build -d

# Wait ~30 seconds for services to initialize, then run demo
make demo
```

### Access the Application

- **Dashboard UI**: http://localhost:3000
- **API Docs**: http://localhost:8000/docs
- **Kafka UI**: http://localhost:8080
- **PostgreSQL**: localhost:5432

**Default Login**: `admin` / `admin`

## ğŸ“‹ Available Commands

```bash
make setup      # Create .env file from template
make up         # Start all services
make down       # Stop all services
make demo       # Run full demo pipeline
make logs       # View logs
make test       # Run tests and linting
make reset      # Wipe all data volumes
```

## ğŸ”„ Running the Demo

The `make demo` command will:

1. Start all services (Kafka, PostgreSQL, Spark, API, UI)
2. Generate 1000 events via the producer
3. Load batch transaction CSVs
4. Run dbt transformations
5. Execute data quality checks

After running, visit http://localhost:3000 to see the dashboard with data.

## ğŸ“Š Dashboard Features

### Overview Page
- Total users, events, revenue
- Conversion rate
- Real-time metrics (events/hour, revenue today)

### Events Analytics
- Real-time event stream visualization
- Events per hour chart (last 24 hours)
- Live indicator for streaming data

### Revenue Analytics
- Daily revenue trends
- Configurable time range (7/30/90 days)
- Bar chart visualization

### Top Products
- Revenue by product
- Quantity and order counts
- Interactive charts and tables

### Data Quality
- Latest Great Expectations checkpoint results
- Pass/fail status
- Failed expectations list

### Pipeline Health
- Pipeline status (streaming, batch)
- Table row counts
- Last run timestamps
- Lag monitoring

## ğŸ—‚ï¸ Project Structure

```
StreamForge/
â”œâ”€â”€ docker-compose.yml          # Orchestration
â”œâ”€â”€ Makefile                     # Convenience commands
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ producer/               # Kafka event producer
â”‚   â”œâ”€â”€ spark/                  # Spark ETL jobs
â”‚   â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â””â”€â”€ ui/                     # Next.js frontend
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ dbt/                    # dbt models and seeds
â”œâ”€â”€ quality/
â”‚   â””â”€â”€ great_expectations/     # Data quality configs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ batch/                  # CSV transaction files
â”‚   â””â”€â”€ schemas/                # JSON schemas
â”œâ”€â”€ scripts/                    # Database init scripts
â””â”€â”€ .github/
    â””â”€â”€ workflows/              # CI/CD pipeline
```

## ğŸ”§ Configuration

All configuration is environment-driven. Copy `.env.example` to `.env` and customize:

```bash
# Database
POSTGRES_USER=streamforge
POSTGRES_PASSWORD=streamforge123
POSTGRES_DB=streamforge

# Kafka
KAFKA_BROKER=kafka:29092
KAFKA_TOPIC=user_events

# API
API_SECRET_KEY=your-secret-key-change-in-production
DEFAULT_ADMIN_USER=admin
DEFAULT_ADMIN_PASSWORD=admin
```

## ğŸ§ª Testing

```bash
# Run API tests
make test

# Or manually
docker compose exec api pytest /app/tests/ -v
docker compose exec api flake8 /app --max-line-length=100
```

## ğŸ“ˆ Adding a New Metric

1. **Create dbt model** (`warehouse/dbt/models/marts/your_metric.sql`)
2. **Add API endpoint** (`services/api/app/metrics.py`)
3. **Add route** (`services/api/app/main.py`)
4. **Create UI page** (`services/ui/src/app/dashboard/your-metric/page.tsx`)
5. **Add nav link** (`services/ui/src/components/Nav.tsx`)

## â˜ï¸ Extending to Cloud

### AWS
- Replace Kafka with MSK
- Use S3 for data lake
- EMR for Spark
- RDS for PostgreSQL
- ECS/Fargate for containers

### GCP
- Replace Kafka with Pub/Sub
- Use Cloud Storage for data lake
- Dataproc for Spark
- Cloud SQL for PostgreSQL
- Cloud Run for containers

### Azure
- Replace Kafka with Event Hubs
- Use Data Lake Storage
- HDInsight for Spark
- Azure SQL for PostgreSQL
- Container Instances for containers

## ğŸ› Troubleshooting

### Kafka won't start
```bash
# Check logs
docker compose logs kafka

# Ensure Zookeeper is healthy
docker compose ps zookeeper
```

### PostgreSQL connection errors
```bash
# Wait for database to be ready
docker compose exec postgres pg_isready -U streamforge

# Check connection from API
docker compose exec api python -c "from app.db import wait_for_db; wait_for_db()"
```

### Port conflicts
If ports 3000, 8000, 8080, or 5432 are in use:
- Stop conflicting services
- Or modify ports in `docker-compose.yml`

### Spark job fails
```bash
# Check Spark logs
docker compose logs spark

# Ensure Kafka is accessible
docker compose exec spark ping kafka
```

### UI not loading
```bash
# Check if API is running
curl http://localhost:8000/health

# Check UI logs
docker compose logs ui

# Rebuild UI
docker compose up --build ui
```

## ğŸ“ Data Model

### Raw Tables
- `raw_events`: All ingested events from Kafka
- `raw_transactions`: Batch-loaded transactions

### Dimension Tables
- `dim_users`: User attributes
- `dim_products`: Product catalog
- `dim_time`: Time dimension

### Fact Tables
- `fact_events`: Curated event data
- `fact_transactions`: Transaction facts with revenue

### Metrics Tables
- `metrics_daily_kpis`: Aggregated daily metrics

## ğŸ” Security Notes

- Default credentials are for **development only**
- Change `API_SECRET_KEY` in production
- Use environment variables for secrets
- Enable HTTPS in production
- Implement proper authentication/authorization

## ğŸ“„ License

MIT License - see LICENSE file

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“§ Support

For issues and questions, please open a GitHub issue.

---

**Built with â¤ï¸ for data engineers**

